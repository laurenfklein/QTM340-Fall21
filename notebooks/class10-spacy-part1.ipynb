{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with spaCy, part 1 \n",
    "\n",
    "*I wrote version 1.0 of this notebook based off materials by Alison Parrish. Dan Sinykin supplemented the 2020 version with material from Melanie Walsh's chapters [Named Entity Recognition](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/Named-Entity-Recognition.html) and [Part-of-Speech Tagging](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/POS-Keywords.html). For 2021, I've added material adapted from David Bamman's [Applied NLP](https://github.com/dbamman/anlp21) course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing spaCy\n",
    "\n",
    "Although NLTK lets you do pretty much anything NLP-related, it can sometimes be difficult to parse through all of the different ways that you might approach a specific task. Enter spaCy, a more recent and more user-friendly NLP library, which has become increasingly widespread, particularly for certain core NLP tasks such as part-of-speech (POS) tagging and named-entitiy recognition (NER). We'll be using both NLTK and spaCy during this course, as do most people, since each have their strengths and weaknesses. \n",
    "\n",
    "One thing to know (and like!) about spaCy is that it relies on machine learning models trained on large amounts of carefully-labeled text. (This is in contrast to the \"large language models\" that we began the course by talking about, which \"learn\" language features in other ways.) The English-language spaCy model that we‚Äôre going to use in this lesson was trained on an annotated corpus called ‚ÄúOntoNotes‚Äù: 2 million+ words drawn from ‚Äúnews, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,‚Äù which were meticulously tagged by a group of researchers and professionals for people‚Äôs names and places, for nouns and verbs, for subjects and objects, and much more. \n",
    "\n",
    "Another thing to know (but maybe not like?!) about spaCY is that, like a lot of other major machine learning projects, OntoNotes was sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military. So, in short, it's complicated! \n",
    "\n",
    "In any case, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the language model \n",
    "\n",
    "I've already downloaded and installed both spaCy and its English-language model (`en_core_web_sm`) on our JupyterHub. This is the model that was trained on the annotated ‚ÄúOntoNotes‚Äù corpus. \n",
    "\n",
    "To load it, we need to do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with spaCy, we first need to process our document with the loaded NLP model. As with sentiment analysis, most of the heavy lifting is done in this single line of code--and not by us, thankfully!\n",
    "\n",
    "After processing, the document object will contain tons of juicy language data ‚Äî named entities, sentence boundaries, parts of speech ‚Äî and the rest of our work will be devoted to accessing this information.\n",
    "\n",
    "Let's create a `Document` object with a few sentences from the Universal Declaration of Human Rights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "\n",
    "spaCy offers an easy way to identify sentences with its `.sents` method. `doc.sents` will turn the doc into a list of sentences. Once you've created a document object, you can iterate over the sentences it contains using the `.sents` attribute as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is the doc: \")\n",
    "print(doc)\n",
    "print(\"\\n\")\n",
    "print(\"And here are the doc's sentences: \")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nerdy Python note: The `.sents` attribute is a [generator](https://wiki.python.org/moin/Generators), so you can't index or count it directly. Generator functions are also known as \"lazy iterators,\" which are objects that you can loop over like a list, but do not store their contents in memory. This may seem like a bug right now, but it's actually a feature because it's a way of saving memory when you're dealing with large amounts of text.\n",
    "\n",
    "To index or count the .sents attribute, you'll need to convert it to a list first using the `list()` function. Then you can treat it like a list and access individual items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_list = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length to make sure it worked\n",
    "\n",
    "print(\"Here's the number of sentences: \" + str(len(sentences_as_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words\n",
    "\n",
    "Iterating over a document (or a sentence) yields each word in turn. Words are represented with spaCy [Token](https://spacy.io/docs/api/token) objects, which have several interesting attributes. \n",
    "\n",
    "The `.text` attribute gives the word, and the `.lemma_` attribute gives the word's \"lemma.\" What is a \"lemma\"?\n",
    "\n",
    "Here are Daniel Jurafsky and James H. Martin on the subject:\n",
    "\n",
    "> **Lemmas and Senses**\n",
    ">\n",
    "> Let‚Äôs start by looking at how one word (we‚Äôll choose \"mouse\") might be defined in a dictionary:\n",
    ">\n",
    "> mouse (N)\n",
    "> 1.  any of numerous small rodents...\n",
    "> 2.  a hand-operated device that controls a cursor...\n",
    ">\n",
    "> Here the form \"mouse\" is the `lemma`, which is also called the \"citation form.\" The form \"mouse\" would also be the lemma for the word \"mice.\" One way to think about it is that dictionaries don‚Äôt have separated definitions for inflected forms like mice. \n",
    ">\n",
    ">Similarly, \"sing\" is the lemma for \"sing,\" \"sang,\" and \"sung.\" In many languages, the infinitive form is used as the lemma for the verb, so Spanish dormir ‚Äúto sleep‚Äù is the lemma for duermes ‚Äúyou sleep‚Äù. The specific forms _sung_ or _carpets_ or _sing_ or _duermes_ are called `wordforms`.\"\n",
    "\n",
    "Let's see how the words and lemmas compare in our tiny document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word, lemma\\n\")\n",
    "for word in doc:\n",
    "    print(word.text + \", \" + word.lemma_)\n",
    "    \n",
    "# Note: On the underscore at the end of a variable, see\n",
    "# https://www.datacamp.com/community/tutorials/role-underscore-python#STU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated above, we can also interate through the document by sentence. To wit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list(doc.sents)[1]  # same as sentence = sentences_as_list[1]\n",
    "\n",
    "for word in sentence:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this distinction between \"word\" and \"lemma\" matter? Well for one, at certain times you might want to be able to gather all of the wordforms of a single word together so that you could count them as a single unit. This will come into play as we start counting words in various ways... \n",
    "\n",
    "But first: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to load data from a file into spaCy\n",
    "\n",
    "You can load data from a file easily with spaCy, which is good because most of the text we'll be analyzing in this course will come to us in file form. When loading a file into spaCy, you just have to make sure that the data is in Unicode format. (Remember our readings from a few classes ago? This is when it starts to become important!). One easy way to do this is to call `.decode('utf8')` on the data after you've loaded it.\n",
    "\n",
    "Let's try this by loading in the text of the 2020 Demcratic platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../corpora/platforms/democrat_platform_2020.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    dem_platform = file.read()\n",
    "print(dem_platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's turn the text into a spaCy document, as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_plat_doc = nlp(dem_platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and do the same thing for the 2020 Republican platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "with open(\"../corpora/platforms/republican_platform_2020.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    repub_platform = file.read()\n",
    "    \n",
    "# turn into a spaCy doc \n",
    "repub_plat_doc = nlp(repub_platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting keywords (with custom code)\n",
    "\n",
    "We'll learn other ways to do this later in the semester, but since this will help us on the way to figuring out why named entity recognition is useful, let's think about a scenario in which we want to know how many times (then) President Trump was named in the Democratic platform vs. the Republican platform, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a basic function to count words\n",
    "\n",
    "def count_words(keyword, document): # this says: we'll always pass in a keyword to search for  \n",
    "                                    # and a spacy document to search \n",
    "    # here's our keyword hit counter\n",
    "    key_count = 0\n",
    "    \n",
    "    # iterate through all the words in the document  \n",
    "    for word in document:\n",
    "        cur_word = word.text # remember, word is an object so you need to look at word.text\n",
    "        \n",
    "        # see if the keyword matches the word, and if so, increment our counter;\n",
    "        if keyword.lower() == cur_word.lower(): # note that this is just a basic string comparison\n",
    "            \n",
    "            # if there's a match, increment the keyword counter by 1 \n",
    "            key_count += 1\n",
    "    \n",
    "    # this returns the final keyword count\n",
    "    return key_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now call our newly defined function for the dem_plat_doc and the repic_plat_doc\n",
    "\n",
    "num_d_trumps = count_words(\"Trump\", dem_plat_doc)\n",
    "num_r_trumps = count_words(\"Trump\", repub_plat_doc)\n",
    "\n",
    "print(\"Number of Trump mentions in Democratic platform: \" + str(num_d_trumps))\n",
    "print(\"Number of Trump mentions in Republican platform: \" + str(num_r_trumps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to see which people were named in each platform, but we didn't know how to search for them by name? This is where NER comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying sentences and words is just the beginning of what spaCy can do. Also encoded in the spaCy `document` is a great deal of information about named entities: that is, about people, places, and many types of things. This is what's known as \"Named Entity Recognition,\" or NER.\n",
    "\n",
    "You might use NER to identify the geographic locations mentioned in texts, a first step toward mapping them. Or you might use NER to identify the most frequently appearing characters in a novel, or to build a network of characters that appear together. Or you might use NER in a more exploratory capacity, as in the scenario I've outlined above.  \n",
    "\n",
    "Below is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify, as well as their corresponding \"type labels.\"\n",
    "\n",
    "Note that \"PERSON\"--what we're going to look for in a sec--is just one of many different named entities that spaCy can recognize. \n",
    "\n",
    "Let's try it out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|\n",
    "|NORP|Nationalities or religious or political groups.|\n",
    "|FAC|Buildings, airports, highways, bridges, etc.|\n",
    "|ORG|Companies, agencies, institutions, etc.|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|Titles of books, songs, etc.|\n",
    "|LAW|Named documents made into laws.|\n",
    "|LANGUAGE|Any named language.|\n",
    "|DATE|Absolute or relative dates or periods.|\n",
    "|TIME|Times smaller than a day.|\n",
    "|PERCENT|Percentage, including ‚Äù%‚Äú.|\n",
    "|MONEY|Monetary values, including unit.|\n",
    "|QUANTITY|Measurements, as of weight or distance.|\n",
    "|ORDINAL|‚Äúfirst‚Äù, ‚Äúsecond‚Äù, etc.|\n",
    "|CARDINAL|Numerals that do not fall under another type.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we pull out all of the people mentioned in the Democratic platform using the `ents` method (short for \"entitites\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an empty list we'll use to store the people\n",
    "people = []\n",
    "    \n",
    "# iterate through all the words in the document (note this could also be sentences) \n",
    "for ent in dem_plat_doc.ents: # this is a method that turns the doc into entitites\n",
    "    if ent.label_ == \"PERSON\": # again, here's basic string comparison\n",
    "        if ent.text not in people: # we'll only add new people\n",
    "            people.append(ent.text)\n",
    "            \n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you modify the code above to pull out the people named in the Republican platform?**\n",
    "\n",
    "**Bonus: Looking at the syntax of the function above, can you write a function that will pull out the people in any document?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete \n",
    "\n",
    "# here's an empty list we can use to store the people\n",
    "people = []\n",
    "    \n",
    "# Iterate through all the words in the document  \n",
    "for ent in repub_plat_doc.ents: # this is a method that turns the doc into entitites\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        if ent.text not in people: # only add new people\n",
    "            people.append(ent.text)\n",
    "            \n",
    "people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a basic function to find people\n",
    "def find_people(document):\n",
    "    # define our list for later use\n",
    "    people = []\n",
    "    \n",
    "    # Iterate through all the words in the document  \n",
    "    for ent in document.ents: # this is a method that turns the doc into entitites\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.text not in people: # only add new people\n",
    "                people.append(ent.text)\n",
    "            \n",
    "    return people\n",
    "\n",
    "find_people(repub_plat_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing spaCy NER with displaCy\n",
    "\n",
    "Here's a neat thing! To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\" to see how spaCy tags its documents. (DisplaCy is special spaCy module for visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we‚Äôre going to need to import displaCy, Also, because the `spaCy` code expects all strings to be unicode strings, per above, we can also include `from __future__ import unicode_literals` to make our lives easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(dem_plat_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, huh? Except, it's not completely perfect...\n",
    "\n",
    "**Can you find some examples of things it tagged properly?**\n",
    "\n",
    "**What about examples of things it misidentified?**\n",
    "\n",
    "(You can scroll through the introductory material if you need to). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse through tags and see how it did -- find correct NEs, incorrect NEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good reminder than spaCy is far from perfect and you always need to check how it does!\n",
    "\n",
    "This is all we're going to do with NER today. As you can see this is just the very tip of the iceberg. You could now easily get, say, a full list of persons or orgs or products from any text. Feel free to explore further on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "\n",
    "One other basic way to analyze text is through what's known as a dictionary. (Not a Python dictionary!). This is just a set of words that evoke some category, optionally paired with a rating on some scale. As I mentioned last class, this is how the first methods of sentiment analysis worked--by counting up the number of \"positive\" and \"negative\" words that matched two predetermined sets of words--or dictionaries to produce the sentiment score. \n",
    "\n",
    "Even though there are now fancier ways to categorize langugage, dictionaries are still widely used--and to sometimes profound effect, as we'll see in our guest lecture next class.\n",
    "\n",
    "The words that constitute a dictionary are determined according to a wide range of methods, from completely hand-crafted, to semi-supervised, to fully \"learned\" (by computers).\n",
    "\n",
    "Let's look at a simple example. \n",
    "\n",
    "Here we'll define our own \"immigration\" dictionary by picking some words that we think have to do with the topic of immigration. (On the scale of hand-crafted to learned, this one is *extremely* hand-crafted). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_dictionary=set([\"wall\", \"border\", \"borders\", \"immigrants\",\"immigration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actully use our previously defined `count_words` function to search both party platforms for how they stack up against the immigration dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_counts = 0\n",
    "repub_counts = 0\n",
    "\n",
    "for word in immigration_dictionary:\n",
    "    dem_counts += count_words(word, dem_plat_doc)\n",
    "    repub_counts += count_words(word, repub_plat_doc)\n",
    "    \n",
    "print(\"Immigratation scores:\")\n",
    "print(\"Dems: \" + str(dem_counts))\n",
    "print(\"Repubs: \" + str(repub_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! For our (flimsy) definition of immmigration, the Democratic platform seem to have mentioned it more than the Republican platform at a ratio of nearly 2:1. \n",
    "\n",
    "**How might you modify this dictionary example in order to assess *how* immigration was discussed in the Democratic vs. Republican platform?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "I've left part-of-speech (PoS) tagging until last because it's simultaneously very useful, very complicated (unless you're a big grammar nerd), and very boring (again, unless you're a big grammar nerd). \n",
    "\n",
    "Why should we bother? Well, for one, parts of speech are the grammatical units of language ‚Äî such as (in English) nouns, verbs, adjectives, adverbs, pronouns, and prepositions. Each of these parts of speech plays a different role in a sentence. By computationally identifying parts of speech, we can start computationally exploring syntax, the relationship between words ‚Äî rather than only focusing on words in isolation.\n",
    "\n",
    "I've attempted to make this lesson more exciting by including some dymanically-generated brightly colored charts and some [xkcd](https://xkcd.com/1443/). Buckle up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/language_nerd.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging with spaCy \n",
    "\n",
    "In spaCy, POS tagging works much likes NER tagging. Here is a chart (which resembles the NER chart) of which parts of speech spaCy is able to recognize and identify:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS   | Description               | Examples                                      |\n",
    "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
    "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
    "| ADP   | adposition                | in, to, during                                |\n",
    "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
    "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
    "| CONJ  | conjunction               | and, or, but                                  |\n",
    "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
    "| DET   | determiner                | a, an, the                                    |\n",
    "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
    "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
    "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
    "| PART  | particle                  | ‚Äôs, not,                                      |\n",
    "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
    "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
    "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
    "| SCONJ | subordinating conjunction | if, while, that                               |\n",
    "| SYM   | symbol                    | $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù             |\n",
    "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
    "| X     | other                     | sfpksdpsxmsa                                  |\n",
    "| SPACE | space                     |                                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the POS for any word using the `pos_` attribute. If you want a more specific designation, you can use the `tag_` attribute.\n",
    "\n",
    "Note that this *is* slightly different than accessing NER tags, which requires that you start with the document's entities (`document.ents`) rather than the document itself. There are technical reasons for this that I can explain if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enough talk. Let's see POS tagging in action!\n",
    "\n",
    "Let's look at the parts-of-speech in the Republican party platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word, POS, tag\\n\")\n",
    "\n",
    "for word in repub_plat_doc:\n",
    "    print(word.text, word.pos_, word.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting words by part of speech\n",
    "\n",
    "Now we can write simple code to extract and recombine words by their part of speech. The following code creates two lists, one for all the nouns and another for all of the adjectives in the Republican platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "for word in repub_plat_doc:\n",
    "    if word.pos_ == 'NOUN' and word.text not in nouns:\n",
    "        nouns.append(word.text)\n",
    "    elif word.pos_ == 'ADJ' and word.text not in adjectives:\n",
    "        adjectives.append(word.text)\n",
    "\n",
    "print(\"here are the first 20 nouns: \" + str(nouns[0:19]))\n",
    "print(\"and here are the first 20 adjectives: \" + str(adjectives[0:19]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below, some code to print out random pairings of an adjective from the text with a noun from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.choice(adjectives) + \" \" + random.choice(nouns))\n",
    "print(random.choice(adjectives) + \" \" + random.choice(nouns))\n",
    "print(random.choice(adjectives) + \" \" + random.choice(nouns))\n",
    "print(random.choice(adjectives) + \" \" + random.choice(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from a few cells up above how the `.tag_` attribute allows us to be even more specific thtan the `.pos` attribute about the parts of speech we want? \n",
    "\n",
    "How would we iterate through the words included in the Republican platform to get a list of only verbs in the past participle?\n",
    "\n",
    "*Hint: you'll find the name of the tag you're looking for [here](https://spacy.io/api/annotation#pos-tagging). Click on \"English\" to expand.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_past_participles = []\n",
    "\n",
    "for word in repub_plat_doc:\n",
    "    # your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger syntactic units\n",
    "\n",
    "So we can get individual words by their part of speech. Great! But what if we want larger chunks, based on their syntactic role in the sentence? The easy way is `.noun_chunks`, which is an attribute of a document or a sentence that evaluates to a list of [spans](https://spacy.io/docs/api/span) of noun phrases, regardless of their position in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dem_plat_doc.noun_chunks:\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anything more sophisticated than this, though, we'll need to learn about how spaCy parses sentences into its syntactic components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dependency grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![displacy parse](http://static.decontextualize.com/syntax_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a dependency grammar is that every word in a sentence is a \"dependent\" on some other word, which is that word's \"head.\" Those \"head\" words are in turn dependents of other words. The finite verb in the sentence is the ultimate \"head\" of the sentence, and is not itself dependent on any other word. (The dependents of a particular head are sometimes called its \"children.\")\n",
    "\n",
    "The question of how to know what constitutes a \"head\" and a \"dependent\" is complicated. As a starting point, here's a passage from [Dependency Grammar and Dependency Parsing](http://stp.lingfil.uu.se/~nivre/docs/05133.pdf):\n",
    "\n",
    "> Here are some of the criteria that have been proposed for identifying a syntactic relation between a head H and a dependent D in a construction C (Zwicky, 1985; Hudson, 1990):\n",
    ">\n",
    "> 1. H determines the syntactic category of C and can often replace C.\n",
    "> 2. H determines the semantic category of C; D gives semantic specification.\n",
    "> 3. H is obligatory; D may be optional.\n",
    "> 4. H selects D and determines whether D is obligatory or optional.\n",
    "> 5. The form of D depends on H (agreement or government).\n",
    "> 6. The linear position of D is specified with reference to H.\"\n",
    "\n",
    "There are different *types* of relationships between heads and dependents, and each type of relation has its own name. \n",
    "\n",
    "**Visit [the displaCy visualizer](https://demos.explosion.ai/displacy/?text=Everyone%20has%20the%20right%20to%20life%2C%20liberty%20and%20security%20of%20person&model=en&cpu=1&cph=0) to see how a particular sentence is parsed, and what the relations between the heads and dependents are.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of a few dependency relations and what they mean. ([A more complete list can be found here.](http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf))\n",
    "\n",
    "* `nsubj`: this word's head is a verb, and this word is itself the subject of the verb\n",
    "* `nsubjpass`: same as above, but for subjects in sentences in the passive voice\n",
    "* `dobj`: this word's head is a verb, and this word is itself the direct object of the verb\n",
    "* `iobj`: same as above, but indirect object\n",
    "* `aux`: this word's head is a verb, and this word is an \"auxiliary\" verb (like \"have\", \"will\", \"be\")\n",
    "* `attr`: this word's head is a copula (like \"to be\"), and this is the description attributed to the subject of the sentence (e.g., in \"This product is a global brand\", `brand` is dependent on `is` with the `attr` dependency relation)\n",
    "* `det`: this word's head is a noun, and this word is a determiner of that noun (like \"the,\" \"this,\" etc.)\n",
    "* `amod`: this word's head is a noun, and this word is an adjective describing that noun\n",
    "* `prep`: this word is a preposition that modifies its head\n",
    "* `pobj`: this word is a dependent (object) of a preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this works in practice\n",
    "# We'll go back to using our first doc, the Universal Declaration of Human Rights, for the rest of this notebook\n",
    "\n",
    "# Let's load it in again in case you're doing this part of the notebook as part of your homework\n",
    "doc = nlp(\"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.\")\n",
    "\n",
    "for word in list(doc.sents)[2]:\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print(\"Dependency relation:\", word.dep_)\n",
    "    print(\"Children:\", list(word.children))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly see spaCy's POS tagging in action by we using the displacy on doc2 with the style= parameter set to \"dep\" (short for dependency parsing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport all our stuff in case you're running this notebook as homework \n",
    "from __future__ import unicode_literals\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "#Set some display options for the visualizer\n",
    "options = {\"compact\": True, \"distance\": 90, \"color\": \"yellow\", \"bg\": \"black\", \"font\": \"Gill Sans\"}\n",
    "\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using .subtree for extracting syntactic units\n",
    "\n",
    "Now that the above makes perfect sense (or your eyes are glazing over), let's learn how the `.subtree` attribute evaluates to a generator (remember that?!) that can be flatted by passing it to `list()`. \n",
    "\n",
    "In this case, the subtree is a list of the word's syntactic dependents--essentially, the clause that the word belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function merges a subtree and returns a string with the text of the words contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "       return ''.join([w.text_with_ws for w in list(st)]).strip() # just take my word for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function in our toolbox, we can write a loop that prints out the subtree for each word in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(doc.sents)[2]:\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Flattened subtree: \", flatten_subtree(word.subtree))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the subtree and our knowledge of dependency relation types, we can write code that extracts larger syntactic units based on their relationship with the rest of the sentence. For example, to get all of the noun phrases that are subjects of a verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        subjects.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or every prepositional phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases = []\n",
    "for word in doc:\n",
    "    if word.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know a large part of how the \"Connotation Frames\" and \"Birth Stories\" projects got made...\n",
    "\n",
    "## Further reading and resources\n",
    "\n",
    "[A few example programs can be found here.](https://github.com/aparrish/rwet-examples/tree/master/spacy)\n",
    "\n",
    "We've barely scratched the surface of what it's possible to do with spaCy. [There's a great page of tutorials on the official site](https://spacy.io/docs/usage/tutorials) that you should check out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
