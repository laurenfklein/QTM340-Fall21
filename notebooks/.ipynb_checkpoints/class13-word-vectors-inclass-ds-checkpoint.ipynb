{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embedding Models: word2vec #\n",
    "\n",
    "*Lauren F. Klein wrote version 1.0 of this notebook based on the [Advanced Topics in Word Vectors workshop](https://dh2018.adho.org/en/machine-reading-part-ii-advanced-topics-in-word-vectors/) at DH 2018 as well as tutorials by [Radim Rehurek](https://rare-technologies.com/word2vec-tutorial/) and [Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why word2vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of this unit, we were introduced to the idea of turning texts into numbers through word vectors. In 2013, Tomas Mikolov and his team at Google used that idea to create word2vec, a word embedding model that is powerful for analyzing text. It is one method for predicting what word is likely to come next in a text; it also allows the exploration of similarity and analogy between words in a corpus. \n",
    "\n",
    "We'll be using gensim, a Python library, to use word2vec today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim is already installed on JupyterHub\n",
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import gensim, nltk tokenizers, glob, and Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim # remember this! \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we need to create our corpus.\n",
    "# we will continue to use the NYT obituaries corpus\n",
    "\n",
    "import os\n",
    "\n",
    "base_dir = \"../docs/NYT-Obituaries/\" \n",
    "\n",
    "all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# lastly, just check the length of all_docs to see if it's 147\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim takes text at the level of `sentences` as its input. \n",
    "\n",
    "So let's define a function that a takes a list of texts (e.g. our all_docs list) and converts it into sentences for gensim word2vec to use. The function will lower-case text and tokenize by sentence and word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need our handy nltk tokenizer \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and we'll get titles\n",
    "directory = \"../docs/NYT-Obituaries/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "obit_titles = [Path(file).stem for file in files]\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "        print(obit_titles[counter]) # let's print the title of the obit\n",
    "        print(len(sentences))  # let's check how many sentences there are per obit\n",
    "        print(\"\\n\")\n",
    "        counter += 1\n",
    "    return all_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it\n",
    "sentences = make_sentences(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our corpus ready for gensim, we can train the model. To do so, we call the function `gensim.models.Word2Vec()`. This function has a couple dozen parameters, some of which are more important than others.\n",
    "\n",
    "Here are a few major ones. Only two are MANDATORY: these are marked with an asterisk:\n",
    "\n",
    "1. `sentences*`: This is where you provide your data. It must be in a format of iterable of iterables.\n",
    "2. `sg`: Your choice of training algorithm. There are two standard ways of training W2V vectors -- 'skipgram' and 'CBOW'. If you enter 1 here the skip-gram is applied; otherwise, the default is CBOW.\n",
    "3. `size*`: This is the length of your resulting word vectors. If you have a large corpus (>few billion tokens) you can go up to 100-300 dimensions. Generally word vectors with more dimensions give better results.\n",
    "4. `window`: This is the window of context words you are training on. In other words, how many words come before and after your given word. A good number is 4 here but this can vary depending on what you are interested in. For instance, if you are more interested in embeddings that embody semantic meaning, smaller window sizes work better. \n",
    "5. `alpha`: The learning rate of your model. If you are interested in machine learning experimentation with your vectors you may experiment with this parameter.\n",
    "6. `seed` (int): This is the random seed for your random initialization. All deep learning models initialize the weights with random floats before training. This is a useful field if you want to replicate your experiments because giving this a seed will initialize 'randomly' deterministically.\n",
    "7. `min_count`: This is the minimum frequency threshold. If a given word appears with lower frequency than provided it will be ignored. This is here because words with very low frequency are hard to train.\n",
    "8. `iter`: This is the number of iterations(entire run) over the corpus, also known as epochs. Usually anything between 1-10 is ok. The trade offs are that if you have higher iterations, it will take longer to train and the model may overfit on your dataset. However, longer training will allow your vectors to perform better on tasks relevant to your dataset.\n",
    "\n",
    "Most of these settings will not concern us. As you'll see below, we are only going to use four arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train our model!\n",
    "obit_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=2, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We have a trained word2vec model: `obit_model`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model — and load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to save your trained model to disk so that you can reload it as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit_model.save('obit_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load a model in the same way (remember this from our topic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = gensim.models.Word2Vec.load('obit_model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `wv.vocab` allows us to see the words in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at the vocab\n",
    "obit_model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adolf Hitler has one of the longest obits in the corpus. word2vec can tell us which words, according to its model, are most similar to any other. We call `model.wv.most_similar(\"word\", topn=number of similar words)`. Let's try `hitler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing some basic functions\n",
    "\n",
    "# basic similarity\n",
    "obit_model.wv.most_similar(\"hitler\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about the results. `stalin` and `mao` make intuitive sense: they are the two mid-twentieth-century authoritarian figures most often compared to Hitler. But what's `irgun`? It's the name of a Zionist paramilitary organization, designated as a terrorist organization by the United Nations, that existed from 1933-1948, during Hitler's rise and fall, and which used violent means to make way for Jews in Palestine by forcing out Palestinians. Our model's parallel invites our critical analysis: rather than take these results as *true*, we need to reflect, in part through turning to obits themselves and close reading them, on how the model arrives at its results, and what they might imply.\n",
    "\n",
    "**Remember**: everything the model knows it knows from our corpus. What we're learning are assumptions *immanent* to the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose specific words to compare with `model.wv.similarity(w1=\"word_one\",w2=\"word_two\")`\n",
    "\n",
    "Let's try with two of Hitler's contemporaries: Marilyn Monroe and Charles Lindbergh. The latter was a famous pilot who was also a Nazi sympathizer and briefly contemplated a political career, a possibility pursued in the counterfactual novel, *The Plot Against America*, which became an HBO show of the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity b/t two words\n",
    "\n",
    "print(obit_model.wv.similarity(w1=\"hitler\",w2=\"marilyn\"))\n",
    "print(obit_model.wv.similarity(w1=\"hitler\",w2=\"lindbergh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model believes Lindbergh is more similar to Hitler than Monroe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also play with analogy tasks. The commonly seen task is:\n",
    "\n",
    "'Man is to King as Woman is to ____?'\n",
    "\n",
    "The general structure is:\n",
    "`A is to A\\*  as  B is to B\\*`\n",
    "                         \n",
    "gensim provides two different ways of implementing this task. You may be familiar with the the additive version also called the 3CosAdd method:\n",
    "\n",
    "$$\\underset{b*\\in V}{\\textrm{arg max}} (cos(b*,b) - cos(b*,a) + cos(b*,a*))$$\n",
    "\n",
    "This reflects the abstraction of Woman - Man + King. In this maximization, we are searching which word vector will allow us to produce the highest value in this equation.\n",
    "\n",
    "We can implement this method with a built-in function. Positive here refers to words that give the positive contribution to similarity (nominator), and negative refers to words that contribute negatively (denominatory). Here's the additive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogies\n",
    "# format is: \"man is to president as woman is to ???\"\n",
    "\n",
    "result = obit_model.wv.most_similar(positive=['woman', 'president'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0])) # this prints the top result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man is to woman as president is to...secretary!? Ah, the bias of the model...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's so much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim has quite a few built-in tools, and it's worth taking some time to see what's available. Check the documentation here: [https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with word2vec in the code cells below, following the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a word and find the 10 words most similar to it\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the results above make sense? Reflect on them. How does attention to the corpus—its particular qualities—help explain what seems to work and what is harder to make sense of?\n",
    "\n",
    "#### ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an analogy after the model \"Man is to King as Woman is to ___\". Try with various possibilities.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experimenting, I found that analogies often don't work terribly well with this corpus. Did they work for you? Why, do you think?\n",
    "\n",
    "#### ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the documentation—https://radimrehurek.com/gensim/models/keyedvectors.html—\n",
    "# and choose a method we haven't yet explored in this notebook\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find below some code you can use to make visualizations from your word2vec model. We can't visualize all the many dimensions in our model, so we need to reduce them to two dimensions for our meager human brains. We do that with something called principal component analysis (PCA). \n",
    "\n",
    "Don't worry about the details for now. This is just a fun way to take a look at the output of our model. \n",
    "\n",
    "**Remember**: Our visualization reduces MANY dimensions to two, so a lot of information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's do some visualization ###\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(obit_model, ['hitler','stalin','lindbergh','marilyn','picasso'])\n",
    "\n",
    "# display_pca_scatterplot(ccp_model, sample=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
