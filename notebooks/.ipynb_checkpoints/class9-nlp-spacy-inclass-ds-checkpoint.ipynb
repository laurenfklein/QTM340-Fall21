{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with spaCy: \n",
    "## Named Entity Recognition (NER) & Part-of-Speech (POS) Tagging\n",
    "\n",
    "*Lauren F. Klein wrote version 1.0, based off a notebook by Alison Parrish. I have supplemented it with material from Melanie Walsh's chapters [Named Entity Recognition](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/Named-Entity-Recognition.html) and [Part-of-Speech Tagging](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/POS-Keywords.html) from her online textbook [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/welcome.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NER useful?\n",
    "\n",
    "NER is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters. Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing?\n",
    "\n",
    "Named Entity Recognition is a fundamental task in the field of natural language processing (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called spellcheck? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "We're going to use an open-source Python library, spaCy, for NER and POS tagging today. spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand).\n",
    "\n",
    "The English-language spaCy model that we‚Äôre going to use in this lesson was trained on an annotated corpus called ‚ÄúOntoNotes‚Äù: 2 million+ words drawn from ‚Äúnews, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,‚Äù which were meticulously tagged by a group of researchers and professionals for people‚Äôs names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n",
    "\n",
    "Okay, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing spaCy\n",
    "\n",
    "From your computer's terminal, type:\n",
    "\n",
    "    !pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the spaCy language model \n",
    "\n",
    "Next we need to download the English-language model (en_core_web_sm), which will be processing and making predictions about our texts. This is the model that was trained on the annotated ‚ÄúOntoNotes‚Äù corpus. You can download the en_core_web_sm model by running the cell below:\n",
    "\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "As above, if you're having trouble with sudo on your machine, just remove the \"sudo\" from the line above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load language model\n",
    "\n",
    "Once the model is downloaded, we need to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We‚Äôre going to import spacy and displacy, a special spaCy module for visualization. The `spaCy` code expects all strings to be unicode strings, so make sure you've included `from __future__ import unicode_literals` at the top of your notebook‚Äîit'll make your life easier, trust me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process our document with the loaded NLP model. Most of the heavy NLP lifting is done in this line of code.\n",
    "\n",
    "After processing, the document object will contain tons of juicy language data ‚Äî named entities, sentence boundaries, parts of speech ‚Äî and the rest of our work will be devoted to accessing this information.\n",
    "\n",
    "Let's create a `Document` object with a few sentences from the Universal Declaration of Human Rights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "\n",
    "spaCy offers an easy way to identify sentences with its `.sents` method. `doc.sents` will turn the doc into a list of sentences. Once you've created a document object, you can iterate over the sentences it contains using the `.sents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is the doc: \")\n",
    "print(doc)\n",
    "print(\"\\n\")\n",
    "print(\"And here are the doc's sentences: \")\n",
    "\n",
    "for item in doc.sents:\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `.sents` attribute is a generator, so you can't index or count it directly. To index or count the .sents attribute, you'll need to convert it to a list first using the `list()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_list = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length to make sure it worked\n",
    "\n",
    "print(\"Here's the number of sentences: \" + str(len(sentences_as_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words\n",
    "\n",
    "Iterating over a document yields each word in turn. Words are represented with spaCy [Token](https://spacy.io/docs/api/token) objects, which have several interesting attributes. \n",
    "\n",
    "The `.text` attribute gives the word, and the `.lemma_` attribute gives the word's \"lemma.\"\n",
    "\n",
    "Here are Daniel Jurafsky and James H. Martin on lemmas:\n",
    "\n",
    "\"**Lemmas and Senses**\n",
    "\n",
    "Let‚Äôs start by looking at how one word (we‚Äôll choose mouse) might be defined in a dictionary:\n",
    "mouse (N)\n",
    "\n",
    "1.  any of numerous small rodents...\n",
    "2.  a hand-operated device that controls a cursor...\n",
    "\n",
    "Here the form mouse is the `lemma`, also called the citation form. The form mouse would also be the lemma for the word mice; dictionaries don‚Äôt have separated efinitions for inflected forms like mice. Similarly sing is the lemma for sing, sang, sung. In many languages the infinitive form is used as the lemma for the verb, so Spanish dormir ‚Äúto sleep‚Äù is the lemma for duermes ‚Äúyou sleep‚Äù. The specific forms _sung_ or _carpets_ or _sing_ or _duermes_ are called `wordforms`.\"\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word, lemma\\n\")\n",
    "for word in doc:\n",
    "    print(word.text + \", \" + word.lemma_)\n",
    "    \n",
    "# Note: On the underscore at the end of a variable, see\n",
    "# https://www.datacamp.com/community/tutorials/role-underscore-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual sentences can also be iterated over to get a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list(doc.sents)[1]  # same as sentence = sentences_as_list[1]\n",
    "\n",
    "for word in sentence:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy & entities\n",
    "\n",
    "Identifying sentences and words is just the beginning of what spaCy can do! Encoded now in the document is a great deal of information about named entities: that is, about people, places, and many types of things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from a file\n",
    "\n",
    "You can load data from a file easily with spaCy. You just have to make sure that the data is in Unicode format, not plain-text. An easy way to do this is to call `.decode('utf8')` on the string after you've loaded it. Let's take a look by loading up one of the songs from our lyrics corpus. Let's try Demi Lovato's \"Made in the USA.\"\n",
    "\n",
    "Remember: **Be sure that you have a folder titled 'lyrics' in the same folder with this notebook on your computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./lyrics/Demi-lovato-made-in-the-usa.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lyrics = file.read()\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's turn the lyrics into a spaCy document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|\n",
    "|NORP|Nationalities or religious or political groups.|\n",
    "|FAC|Buildings, airports, highways, bridges, etc.|\n",
    "|ORG|Companies, agencies, institutions, etc.|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|Titles of books, songs, etc.|\n",
    "|LAW|Named documents made into laws.|\n",
    "|LANGUAGE|Any named language.|\n",
    "|DATE|Absolute or relative dates or periods.|\n",
    "|TIME|Times smaller than a day.|\n",
    "|PERCENT|Percentage, including ‚Äù%‚Äú.|\n",
    "|MONEY|Monetary values, including unit.|\n",
    "|QUANTITY|Measurements, as of weight or distance.|\n",
    "|ORDINAL|‚Äúfirst‚Äù, ‚Äúsecond‚Äù, etc.|\n",
    "|CARDINAL|Numerals that do not fall under another type.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc2, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, spaCy can identify \"every day\" and \"Winter\" as `DATE`s, West Coast and East Coast as `LOC`s, and USA as a `GPE`. It recognizes Chevy but thinks its an `ORG`, the company Chevrolet, rather than a `PRODUCT`. This is a good reminder than spaCy is far from perfect and you always need to check how it does!\n",
    "\n",
    "This is all we're going to do with NER today. As you can see this is just the very tip of the iceberg. You could now easily get, say, a full list of persons or orgs or products from any text. Feel free to explore further on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech\n",
    "\n",
    "In this part of the lesson, we're going to learn about the textual analysis method _part-of-speech tagging_.  \n",
    "\n",
    "spaCy's `pos_` attribute gives a general part of speech; the `tag_` attribute gives a more specific designation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Part-of-Speech Tagging Useful?\n",
    "\n",
    "I don't mean to go all [Language Nerd](https://xkcd.com/1443/) on you, but parts of speech are important. Even if they seem kind of boring. *Parts of speech* are the grammatical units of language ‚Äî such as (in English) nouns, verbs, adjectives, adverbs, pronouns, and prepositions. Each of these parts of speech plays a different role in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/language_nerd.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computationally identifying parts of speech, we can start computationally exploring *syntax*, the relationship between words ‚Äî rather than only focusing on words in isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Part-of-Speech Tagging\n",
    "\n",
    "What **parts** of speech does spaCy recognize and identify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| POS   | Description               | Examples                                      |\n",
    "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
    "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
    "| ADP   | adposition                | in, to, during                                |\n",
    "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
    "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
    "| CONJ  | conjunction               | and, or, but                                  |\n",
    "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
    "| DET   | determiner                | a, an, the                                    |\n",
    "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
    "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
    "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
    "| PART  | particle                  | ‚Äôs, not,                                      |\n",
    "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
    "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
    "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
    "| SCONJ | subordinating conjunction | if, while, that                               |\n",
    "| SYM   | symbol                    | $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù             |\n",
    "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
    "| X     | other                     | sfpksdpsxmsa                                  |\n",
    "| SPACE | space                     |                                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a POS chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different parts of speech that spaCy can identify as well as their corresponding labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see it in action!\n",
    "\n",
    "We'll check out the parts-of-speech in Demi Lovato's 'Made in the USA', which, remember, is defined as `doc2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word, POS, tag\\n\")\n",
    "\n",
    "for item in doc2:\n",
    "    print(item.text, item.pos_, item.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly see spaCy's POS tagging in action by we using the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) on `doc2` with the `style=` parameter set to \"dep\" (short for dependency parsing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some display options for the visualizer\n",
    "options = {\"compact\": True, \"distance\": 90, \"color\": \"yellow\", \"bg\": \"black\", \"font\": \"Gill Sans\"}\n",
    "\n",
    "displacy.render(doc2, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting words by part of speech\n",
    "\n",
    "Now we can write simple code to extract and recombine words by their part of speech. The following code creates lists of all nouns and adjectives in Lovato's song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for item in doc2:\n",
    "    if item.pos_ == 'NOUN' and item.text not in nouns:\n",
    "        nouns.append(item.text)\n",
    "for item in doc2:\n",
    "    if item.pos_ == 'ADJ' and item.text not in adjectives:\n",
    "        adjectives.append(item.text)\n",
    "print(\"here are the nouns: \" + str(nouns))\n",
    "print(\"and here are the adjectives: \" + str(adjectives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below, some code to print out random pairings of an adjective from the text with a noun from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.choice(adjectives) + \" \" + random.choice(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a list of verbs works similarly. You try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.tag_` attribute allows us to be more specific about the kinds of verbs we want. \n",
    "\n",
    "**HOW WOULD WE GET ONLY VERBS IN PAST PARTICIPLE?**\n",
    "_Hint: you'll find help [here](https://spacy.io/api/annotation#pos-tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_past = []\n",
    "\n",
    "for item in doc2:\n",
    "    # THEN WHAT???\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "only_past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger syntactic units\n",
    "\n",
    "So we can get individual words by their part of speech. Great! But what if we want larger chunks, based on their syntactic role in the sentence? The easy way is `.noun_chunks`, which is an attribute of a document or a sentence that evaluates to a list of [spans](https://spacy.io/docs/api/span) of noun phrases, regardless of their position in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in doc2.noun_chunks:\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anything more sophisticated than this, though, we'll need to learn about how spaCy parses sentences into its syntactic components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dependency grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![displacy parse](http://static.decontextualize.com/syntax_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a dependency grammar is that every word in a sentence is a \"dependent\" on some other word, which is that word's \"head.\" Those \"head\" words are in turn dependents of other words. The finite verb in the sentence is the ultimate \"head\" of the sentence, and is not itself dependent on any other word. (The dependents of a particular head are sometimes called its \"children.\")\n",
    "\n",
    "The question of how to know what constitutes a \"head\" and a \"dependent\" is complicated. As a starting point, here's a passage from [Dependency Grammar and Dependency Parsing](http://stp.lingfil.uu.se/~nivre/docs/05133.pdf):\n",
    "\n",
    "> Here are some of the criteria that have been proposed for identifying a syntactic relation between a head H and a dependent D in a construction C (Zwicky, 1985; Hudson, 1990):\n",
    ">\n",
    "> 1. H determines the syntactic category of C and can often replace C.\n",
    "> 2. H determines the semantic category of C; D gives semantic specification.\n",
    "> 3. H is obligatory; D may be optional.\n",
    "> 4. H selects D and determines whether D is obligatory or optional.\n",
    "> 5. The form of D depends on H (agreement or government).\n",
    "> 6. The linear position of D is specified with reference to H.\"\n",
    "\n",
    "There are different *types* of relationships between heads and dependents, and each type of relation has its own name. \n",
    "\n",
    "**Visit [the displaCy visualizer](https://demos.explosion.ai/displacy/?text=Everyone%20has%20the%20right%20to%20life%2C%20liberty%20and%20security%20of%20person&model=en&cpu=1&cph=0) to see how a particular sentence is parsed, and what the relations between the heads and dependents are.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of a few dependency relations and what they mean. ([A more complete list can be found here.](http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf))\n",
    "\n",
    "* `nsubj`: this word's head is a verb, and this word is itself the subject of the verb\n",
    "* `nsubjpass`: same as above, but for subjects in sentences in the passive voice\n",
    "* `dobj`: this word's head is a verb, and this word is itself the direct object of the verb\n",
    "* `iobj`: same as above, but indirect object\n",
    "* `aux`: this word's head is a verb, and this word is an \"auxiliary\" verb (like \"have\", \"will\", \"be\")\n",
    "* `attr`: this word's head is a copula (like \"to be\"), and this is the description attributed to the subject of the sentence (e.g., in \"This product is a global brand\", `brand` is dependent on `is` with the `attr` dependency relation)\n",
    "* `det`: this word's head is a noun, and this word is a determiner of that noun (like \"the,\" \"this,\" etc.)\n",
    "* `amod`: this word's head is a noun, and this word is an adjective describing that noun\n",
    "* `prep`: this word is a preposition that modifies its head\n",
    "* `pobj`: this word is a dependent (object) of a preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how this works in practice\n",
    "# We'll go back to using our first doc for the rest of this notebook\n",
    "\n",
    "for word in list(doc.sents)[2]:\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print(\"Dependency relation:\", word.dep_)\n",
    "    print(\"Children:\", list(word.children))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using .subtree for extracting syntactic units\n",
    "\n",
    "The `.subtree` attribute evaluates to a generator that can be flatted by passing it to `list()`. This is a list of the word's syntactic dependents--essentially, the clause that the word belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function merges a subtree and returns a string with the text of the words contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "       return ''.join([w.text_with_ws for w in list(st)]).strip() # just take my word for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function in our toolbox, we can write a loop that prints out the subtree for each word in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(doc.sents)[2]:\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Flattened subtree: \", flatten_subtree(word.subtree))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the subtree and our knowledge of dependency relation types, we can write code that extracts larger syntactic units based on their relationship with the rest of the sentence. For example, to get all of the noun phrases that are subjects of a verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        subjects.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or every prepositional phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases = []\n",
    "for word in doc:\n",
    "    if word.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and resources\n",
    "\n",
    "[A few example programs can be found here.](https://github.com/aparrish/rwet-examples/tree/master/spacy)\n",
    "\n",
    "We've barely scratched the surface of what it's possible to do with spaCy. [There's a great page of tutorials on the official site](https://spacy.io/docs/usage/tutorials) that you should check out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
