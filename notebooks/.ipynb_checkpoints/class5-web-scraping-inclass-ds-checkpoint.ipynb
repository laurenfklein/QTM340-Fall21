{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup\n",
    "\n",
    "Lauren F. Klein wrote version 1.0 of this notebook, based on lessons by [Alison Parrish](http://www.decontextualize.com/) and [Jinho Choi](https://github.com/emory-courses/data-science/blob/master/course/data_aggregation/data_aggregation.ipynb), which I have supplemented with material from Melanie Walsh's chapter [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF.html) from her online textbook [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/welcome.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need To Scrape At All?\n",
    "\n",
    "To perform practical approaches to data science with text, we need...**text**. Some text is prepared for computational analysis and publicly available in digital libraries. We can easily get novels in the public domain as .txt files, for example, from [Project Gutenberg](https://www.gutenberg.org/). Or we can work with text from HathiTrust's vast collections through the [HathiTrust Research Center](https://analytics.hathitrust.org/). \n",
    "\n",
    "If we want to work with text from popular culture and the internet--and we do!--we often need to scrape the web. To understand the necessity and significance of web scraping, let's walk through the likely data collection process behind [‚ÄúFilm Dialogue from 2,000 screenplays, Broken Down by Gender and Age‚Äù](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
    "\n",
    "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import pandas, a Python library for managing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we read in the data, which is stored as a .csv file. First you have to download the file from Canvas and store it somewhere where you know where to find it. I've stored my in the `docs` folder within my `QTM-340` folder, which contains the materials for this course. Go to Canvas now to download it or get there by clicking [here](https://canvas.emory.edu/courses/76593). It's called `raw_script_urls` and it's under the `Datasets` module.\n",
    "\n",
    "You'll see three \"arguments\" between parentheses in the code below. The first is the filepath, telling pandas where to find the data. You will have to edit this argument to match where you've stored the data on your computer. Don't worry about the next two arguments for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"../docs/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the code naming the data we've just read in, Jupyter will render the top and bottom five rows from the .csv for us. It will also tell us how many rows and columns are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremely useful dataset! But how can we actually use these URLs to get workable, computationally tractable text data? Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses and Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
    "\n",
    "**Side note:** Since you're familiar with R, you know what a function is: a block of organized, reusable code that is called by a name, and performs some sort of action. Python has functions too, as well as things called methods, which are functions that are associated only with a particular object or class. Keep on reading to see an example of the \n",
    "`get` method in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Status Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the `.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! Here's the screenplay now in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls = urls.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_screenplay(url):\n",
    "    response = requests.get(url)\n",
    "    html_string = response.text\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in sample_urls['script_url']:\n",
    "    full_screenplay = scrape_screenplay(url)\n",
    "    sample_screenplay = full_screenplay[:900]\n",
    "    print(f\"\\nüé¨üé¨üé¨üé¨üé¨üé¨üé¨\\n{sample_screenplay}\\nüé¨üé¨üé¨üé¨üé¨üé¨üé¨\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! We now know how to scrape text from simple websites! We can get a lot of text data this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup & HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all web pages will be as easy to scrape as these screenplay files. Let's say we wanted to scrape the lyrics for Missy Elliott's song \"The Rain (Supa Dupa Fly)\" (1997) from *Genius*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../web-scraping/images/Missy-Elliott.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Ghostbusters* page and that it contains a lot of information beyond the lyrics. Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
    "html_string = response.text\n",
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called BeautifulSoup, which parses HTML documents.\n",
    "\n",
    "To understand BeautifulSoup and HTML, we're going to briefly depart from our Missy Elliot lyrics challenge to consider a much simpler website. (But we will return to Missy soon!) This toy website was made by the poet, programmer, and professor Allison Parrish explicitly for the purposes of teaching BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parrish's website is titled \"Kittens and the TV Shows They Love,\" and it can be found at the following URL: http://static.decontextualize.com/kittens.html Let's check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use our requests library on this Kittens TV website, this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
    "html_string = response.text\n",
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an HTML document. HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| HTML Tag                | Explanation                              |\n",
    "|--------------------|-------------------------------------------|\n",
    "| <\\!DOCTYPE>        | Defines document type                 |\n",
    "| <html\\>             | Defines HTML document                  |\n",
    "| <head\\>             | Main information about document    |\n",
    "| <title\\>            | Title for document          |\n",
    "| <body\\>             | Document body               |\n",
    "| <h1\\> to <h6\\>       |  Headings                    |\n",
    "| <p\\>                | Paragraph                       |\n",
    "| <br\\>               | Line break               |\n",
    "| <\\!\\-\\-comment here-\\-> | Comment                         |\n",
    "| <img\\> | Image                         |\n",
    "| <a\\> | Hyperlink                       |\n",
    "| <ul\\> | Unordered list                     |\n",
    "| <ol\\> | Ordered list                     |\n",
    "| <li\\> | List item                     |\n",
    "| <style\\> | Style information for a document                    |\n",
    "| <div\\> | Section in a document                   |\n",
    "| <span\\> | Section in a document                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Kittens and the TV Shows They Love</h1>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Attributes, Classes, and IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML elements sometimes come with even more information inside a tag. This will often be a keyword (like `class` or `id`) followed by an equals sign `=` and a further descriptor such as `<div class=\"kitten\">`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know about tags as well as attributes, classes, and IDs because this is how we're going to extract specific HTML data with BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again is source code for kittens.html:\n",
    "\n",
    "\t<!doctype html>\n",
    "\t<html>\n",
    "\t  <head>\n",
    "\t    <title>Kittens!</title>\n",
    "\t  </head>\n",
    "\t  <body>\n",
    "\t    <h1>Kittens and the TV Shows They Love</h1>\n",
    "\t    <div class=\"kitten\">\n",
    "\t      <h2>Fluffy</h2>\n",
    "\t      <div><img src=\"http://placekitten.com/100/100\"></div>\n",
    "\t      <ul class=\"tvshows\">\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0106145/\">Deep Space Nine</a></li>\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0088576/\">Mr. Belvedere</a></li>\n",
    "\t      </ul>\n",
    "\t      Last check-up: <span class=\"lastcheckup\">2014-01-17</span>\n",
    "\t    </div>\n",
    "\t    <div class=\"kitten\">\n",
    "\t      <h2>Monsieur Whiskeurs</h2>\n",
    "\t      <div><img src=\"http://placekitten.com/150/100\"></div>\n",
    "\t      <ul class=\"tvshows\">\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0106179/\">The X-Files</a></li>\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0098800/\">Fresh Prince</a></li>\n",
    "\t      </ul>\n",
    "\t      Last check-up: <span class=\"lastcheckup\">2013-11-02</span>\n",
    "\t    </div>\n",
    "\t  </body>\n",
    "\t</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty well organized HTML, but if you don't know how to read HTML, it will still look like a big jumble. Here's how I would characterize the structure of this HTML, reading in my own idea of what the meaning of the elements are.\n",
    "\n",
    "* We have two \"kittens,\" both of which are contained in `<div>` tags with class `kitten`.\n",
    "* Each \"kitten\" `<div>` has an `<h2>` tag with that kitten's name.\n",
    "* There's an image for each kitten, specified with an `<img>` tag.\n",
    "* Each kitten has a list (a `<ul>` with class `tvshows`) of television shows, contained within `<li>` tags.\n",
    "* Those list items themselves have links (`<a>` tags) with an `href` attribute that contains a link to an IMDB entry for that show.\n",
    "\n",
    "**SOME HTML QUESTIONS FOR YOU:**\n",
    "* What's the parent tag of `<a href=\"http://www.imdb.com/title/tt0088576/\">Mr. Belvedere</a>`? \n",
    "\n",
    "* Both `<div class=\"kitten\">` tags share a parent tag---what is it? What attributes are present on both `<img>` tags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping kittens with Beautiful Soup\n",
    "\n",
    "We've examined `kittens.html` a bit now. What we'd like to do is write some code that is going to extract information from the HTML, like \"what is the last checkup date for each of these kittens?\" or \"what are Monsieur Whiskeur's favorite TV shows?\" To do so, we need to *parse* the HTML, and create a representation of it in our program that we can manipulate with Python.\n",
    "\n",
    "As mentioned earlier, HTML is hard to parse by hand. (Don't even try it. In particular, [don't parse HTML with regular expressions](http://stackoverflow.com/a/1732454).)\n",
    "\n",
    "Let's scrape text from html!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) that we want to use, which will always be `\"html.parser\"` for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
    "html_string = response.text\n",
    "\n",
    "document = BeautifulSoup(html_string, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.find()` HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.find()` method to find and extract certain elements, such as a main header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want only the text contained between those tags, we can use `.text` to extract just the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h1\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(document.find(\"h1\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the HTML element that contains an image. Hint: the HTML image tag is \"img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You might have noticed that there is more than one `<img>` tag in `kittens.html`! If more than one tag matches the name you pass to `.find()`, it returns only the *first* matching tag. (A better name for `.find()` might be `find_first`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.find_all()` HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract multiple HTML elements at a time with `.find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"div\", attrs={\"class\": \"kitten\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h2\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to extact the text from all the header2 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"h2\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh. That didn't work! To extract text data from multiple HTML elements, we need a `for` loop and some list-building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store all the headers in a list\n",
    "all_h2_headers = document.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we just want to see the text of the headers?\n",
    "\n",
    "To find out, we can use a loop.\n",
    "\n",
    "We're going to use some 'for' loop syntax that's very common in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in all_h2_headers:\n",
    "    print(tag.string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we'll need to built a list with just the text from the headers. Here's how.\n",
    "\n",
    "First we will make an empty list called `h2_headers`. \n",
    "\n",
    "Then `for` each `header` in `all_h2_headers`, we will grab the `.text`, put it into a variable called `header_contents`, then `.append()` it to our `h2_headers` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers = []\n",
    "for header in all_h2_headers:\n",
    "    header_contents = header.text\n",
    "    h2_headers.append(header_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we transform this exact same for loop into a one line? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers = [header.text for header in all_h2_headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note about list comprehension in Python**\n",
    "\n",
    "The one line version is a helpful shorthand: it creates a list (same as R lists) with each of the `header.text`s in `all_h2_headers`.\n",
    "\n",
    "In more official terms, it's called a *list comprehension*, and it helps with a very common task in both data analysis and computer programming: when you want to apply an operation to every item in a list (e.g., scaling the numbers in a list by a fixed factor), or create a copy of a list with only those items that match a particular criterion (e.g., eliminating values that fall below a certain threshold). \n",
    "\n",
    "A list comprehension has a few parts:\n",
    "\n",
    "* a source list, or the list whose values will be transformed or filtered;\n",
    "* a predicate expression, to be evaluated for every item in the list;\n",
    "* (optionally) a membership expression that determines whether or not an item in the source list will be included in the result of evaluating the list comprehension, based on whether the expression evaluates to True or False; and\n",
    "* a temporary variable name by which each value from the source list will be known in the predicate expression and membership expression.\n",
    "These parts are arranged like so:\n",
    "\n",
    "> `[` *predicate expression* `for` *temporary variable name* `in` *source list* `if` *membership expression* `]`\n",
    "\n",
    "The words for, in, and if are a part of the syntax of the expression. They don't mean anything in particular (and in fact, they do completely different things in other parts of the Python language). You just have to spell them right and put them in the right place in order for the list comprehension to work.\n",
    "\n",
    "You can see more examples of this in action [here](lists.ipynb).\n",
    "\n",
    "**An unrelated note, but before we move on:**\n",
    "\n",
    "Beautiful Soup's `.find()` and `.find_all()` methods are actually more powerful than we're letting on here. [Check out the details in the official Beautiful Soup documentation.](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a slightly more complicated version of a list comprehension.\n",
    "\n",
    "Both the `.find()` and `.find_all()` methods can search not just for tags with particular names, but also for tags that have particular attributes. For that, we use the `attrs` keyword argument, giving it a dictionary that associates attribute names as keys and the desired attribute value as values. For example, to find all `span` tags with a `class` attribute of `lastcheckup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkup_tags = document.find_all('span', attrs={'class': 'lastcheckup'})\n",
    "[tag.string for tag in checkup_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting HTML üßê\n",
    "\n",
    "It is often helpful to inspect the HTML from the rendered site to get a sense of how to scrape what you want to scrape.\n",
    "\n",
    "Click on the link: http://static.decontextualize.com/kittens.html. In most browsers, you can ctrl-click (or right click) anywhere on the page and select \"Inspect Element.\" Go ahead. Your screen should look (something) like this:\n",
    "\n",
    "<a href=\"http://static.decontextualize.com/snaps/kittens-dev-tools.png\"><img src=\"http://static.decontextualize.com/snaps/kittens-dev-tools.png\" alt=\"kittens-dev-tools\"/></a>\n",
    "\n",
    "In the upper panel, you see the web page you're inspecting. In the lower panel, you see a version of the HTML source code, with little arrows next to some of the lines. (The little arrows allow you to collapse parts of the HTML source that are hierarchically related.) As you move your mouse over the elements in the top panel, different parts of the source code will be highlighted. Your browswer is showing you which parts of the source code are causing which parts of the page to show up. Pretty spiffy!\n",
    "\n",
    "This relationship also works in reverse: you can move your mouse over some part of the source code in the lower panel, which will highlight in the top panel what that source code corresponds to on the page. We can use this to visually identify the parts of the page that are interesting to us, so we can write code that extracts the contents of those parts automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Missy Elliott ‚Äî Your Turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we've learned a little bit about how to use BeautifulSoup to parse HTML documents. So how would we apply what we've learned to extract Missy Elliott lyrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
    "html_str = response.text\n",
    "\n",
    "document = BeautifulSoup(html_str, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What HTML element do we need to \"find\" to extract the song lyrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missy_lyrics = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check answer below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missy_lyrics = document.find(\"p\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missy_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What HTML element do we need to \"find\" to extract the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_title = document.find(\"h1\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(song_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONGRATULATIONS!!** \n",
    "\n",
    "You now know how to scrape the web! Endless text is now available for your data science needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
